{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83eab20",
   "metadata": {},
   "source": [
    "Laboratorio 3 Noemí Pino (Proceso ETL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c4d8b",
   "metadata": {},
   "source": [
    "FASE 1: EXTRACCIÓN\n",
    "\n",
    "Fuente: Dataset público de Kaggle: Dirty Cafe Sales Dataset\n",
    "Archivo: dirty_cafe_sales.csv\n",
    "Registros: 10.000\n",
    "Columnas: 8\n",
    "\n",
    "Justificación de la fuente\n",
    "\n",
    "Elegí este dataset porque:\n",
    "\n",
    "    1. Tiene varios registros y lo más parecido a un registro real  \n",
    "\n",
    "    2. Es un dataset público y sintético, diseñado específicamente para practicar limpieza y transformación (ETL).\n",
    "\n",
    "    3. Contiene valores faltantes, inválidos e inconsistentes, lo que lo hace ideal para demostrar manejo de datos sucios, imputaciones, estandarización y enriquecimiento.S\n",
    "\n",
    "    4. Su estructura (ventas de cafetería) es fácilmente entendible y aplicable a entornos reales de negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b07581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b694fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas y columnas: (10000, 8)\n",
      "\n",
      "Tipos de datos por columna:\n",
      " Transaction ID      object\n",
      "Item                object\n",
      "Quantity            object\n",
      "Price Per Unit      object\n",
      "Total Spent         object\n",
      "Payment Method      object\n",
      "Location            object\n",
      "Transaction Date    object\n",
      "dtype: object\n",
      "\n",
      "Primeras filas del dataset:\n",
      "  Transaction ID    Item Quantity Price Per Unit Total Spent  Payment Method  \\\n",
      "0    TXN_1961373  Coffee        2            2.0         4.0     Credit Card   \n",
      "1    TXN_4977031    Cake        4            3.0        12.0            Cash   \n",
      "2    TXN_4271903  Cookie        4            1.0       ERROR     Credit Card   \n",
      "3    TXN_7034554   Salad        2            5.0        10.0         UNKNOWN   \n",
      "4    TXN_3160411  Coffee        2            2.0         4.0  Digital Wallet   \n",
      "\n",
      "   Location Transaction Date  \n",
      "0  Takeaway       2023-09-08  \n",
      "1  In-store       2023-05-16  \n",
      "2  In-store       2023-07-19  \n",
      "3   UNKNOWN       2023-04-27  \n",
      "4  In-store       2023-06-11  \n",
      "\n",
      "Valores nulos por columna:\n",
      "Transaction ID         0\n",
      "Item                 333\n",
      "Quantity             138\n",
      "Price Per Unit       179\n",
      "Total Spent          173\n",
      "Payment Method      2579\n",
      "Location            3265\n",
      "Transaction Date     159\n",
      "dtype: int64\n",
      "\n",
      "Valores más comunes en Transaction ID:\n",
      "Transaction ID\n",
      "TXN_1961373    1\n",
      "TXN_4977031    1\n",
      "TXN_4271903    1\n",
      "TXN_7034554    1\n",
      "TXN_3160411    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Item:\n",
      "Item\n",
      "Juice       1171\n",
      "Coffee      1165\n",
      "Salad       1148\n",
      "Cake        1139\n",
      "Sandwich    1131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Quantity:\n",
      "Quantity\n",
      "5    2013\n",
      "2    1974\n",
      "4    1863\n",
      "3    1849\n",
      "1    1822\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Price Per Unit:\n",
      "Price Per Unit\n",
      "3.0    2429\n",
      "4.0    2331\n",
      "2.0    1227\n",
      "5.0    1204\n",
      "1.0    1143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Total Spent:\n",
      "Total Spent\n",
      "6.0     979\n",
      "12.0    939\n",
      "3.0     930\n",
      "4.0     923\n",
      "20.0    746\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Payment Method:\n",
      "Payment Method\n",
      "NaN               2579\n",
      "Digital Wallet    2291\n",
      "Credit Card       2273\n",
      "Cash              2258\n",
      "ERROR              306\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Location:\n",
      "Location\n",
      "NaN         3265\n",
      "Takeaway    3022\n",
      "In-store    3017\n",
      "ERROR        358\n",
      "UNKNOWN      338\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valores más comunes en Transaction Date:\n",
      "Transaction Date\n",
      "UNKNOWN       159\n",
      "NaN           159\n",
      "ERROR         142\n",
      "2023-06-16     40\n",
      "2023-02-06     40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dirty_cafe_sales.csv\") #leer el archivo CSV\n",
    "\n",
    "#Revisar estructura general\n",
    "print(\"Filas y columnas:\", df.shape)        # tamaño del dataset\n",
    "print(\"\\nTipos de datos por columna:\\n\", df.dtypes)\n",
    "\n",
    "#Ver las primeras filas\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(df.head(5))\n",
    "\n",
    "#Ver cantidad de valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#Comprobar si existen valores atípicos comunes (\"ERROR\", \"UNKNOWN\")\n",
    "for col in df.columns:\n",
    "    print(f\"\\nValores más comunes en {col}:\")\n",
    "    print(df[col].value_counts(dropna=False).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737d825",
   "metadata": {},
   "source": [
    "FASE 2: TRANSFORMACIÓN\n",
    "\n",
    "Objetivo: Limpiar y transformar los datos del dataset para:\n",
    "\n",
    "1. Eliminar o reemplazar valores inválidos (ERROR, UNKNOWN, vacíos).\n",
    "\n",
    "2. Normalizar tipos de datos (numéricos, fechas).\n",
    "\n",
    "3. Corregir totales inconsistentes (Total Spent).\n",
    "\n",
    "4. Crear nuevas columnas (enriquecimiento).\n",
    "\n",
    "5. Dejar los datos listos para cargarse a MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce3e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction ID         0\n",
      "Item                 969\n",
      "Quantity             479\n",
      "Price Per Unit       533\n",
      "Total Spent          502\n",
      "Payment Method      3178\n",
      "Location            3961\n",
      "Transaction Date     460\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PASO 1: REEMPLAZO DE VALORES INVÁLIDOS\n",
    "\n",
    "#Reemplazamos 'ERROR', 'UNKNOWN' y espacios vacíos por NaN\n",
    "df = df.replace([\"ERROR\", \"UNKNOWN\", \" \"], pd.NA)\n",
    "\n",
    "#Verificamos el cambio\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a50c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity          float64\n",
      "Price Per Unit    float64\n",
      "Total Spent       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#PASO 2: CONVERSIÓN DE TIPOS\n",
    "\n",
    "#Convertimos columnas numéricas a tipo float\n",
    "num_cols = [\"Quantity\", \"Price Per Unit\", \"Total Spent\"]\n",
    "for col in num_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "#Revisamos los nuevos tipos\n",
    "print(df[num_cols].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e01389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totales corregidos: 40\n"
     ]
    }
   ],
   "source": [
    "#PASO 3: CORRECCIÓN DE TOTAL SPENT\n",
    "\n",
    "#Si hay valores válidos en Quantity y Price, recalculamos el total\n",
    "df[\"Calculated Total\"] = df[\"Quantity\"] * df[\"Price Per Unit\"]\n",
    "\n",
    "#Rellenamos Total Spent cuando está vacío\n",
    "df[\"Total Spent\"] = df[\"Total Spent\"].fillna(df[\"Calculated Total\"])\n",
    "\n",
    "#Verificamos cuántos valores se corrigieron\n",
    "print(\"Totales corregidos:\", df[\"Total Spent\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed46a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transaction Date Transaction Month Day of Week\n",
      "0       2023-09-08           2023-09      Friday\n",
      "1       2023-05-16           2023-05     Tuesday\n",
      "2       2023-07-19           2023-07   Wednesday\n",
      "3       2023-04-27           2023-04    Thursday\n",
      "4       2023-06-11           2023-06      Sunday\n"
     ]
    }
   ],
   "source": [
    "# PASO 4: LIMPIEZA Y DERIVACIÓN DE FECHAS\n",
    "\n",
    "#Convertir la columna a datetime (los inválidos se vuelven NaT)\n",
    "df[\"Transaction Date\"] = pd.to_datetime(df[\"Transaction Date\"], errors=\"coerce\")\n",
    "\n",
    "#Crear columnas derivadas para análisis\n",
    "df[\"Transaction Month\"] = df[\"Transaction Date\"].dt.to_period(\"M\").astype(str)\n",
    "df[\"Day of Week\"] = df[\"Transaction Date\"].dt.day_name()\n",
    "\n",
    "#Mostrar ejemplo\n",
    "print(df[[\"Transaction Date\", \"Transaction Month\", \"Day of Week\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea989412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas después de eliminar duplicados: 10000\n"
     ]
    }
   ],
   "source": [
    "#PASO 5: ELIMINAR DUPLICADOS\n",
    "\n",
    "#Eliminamos duplicados basados en Transaction ID\n",
    "df = df.drop_duplicates(subset=\"Transaction ID\")\n",
    "\n",
    "# Confirmamos cantidad de filas después\n",
    "print(\"Filas después de eliminar duplicados:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5828aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Item  Expected Price  Price Per Unit  Price Deviation  Category\n",
      "0    Coffee             2.0             2.0              0.0  Beverage\n",
      "1      Cake             3.0             3.0              0.0      Food\n",
      "2    Cookie             1.0             1.0              0.0      Food\n",
      "3     Salad             5.0             5.0              0.0      Food\n",
      "4    Coffee             2.0             2.0              0.0  Beverage\n",
      "5  Smoothie             4.0             4.0              0.0  Beverage\n",
      "6      <NA>             NaN             3.0              NaN      Food\n",
      "7  Sandwich             4.0             4.0              0.0      Food\n",
      "8       NaN             NaN             3.0              NaN      Food\n",
      "9  Sandwich             4.0             4.0              0.0      Food\n"
     ]
    }
   ],
   "source": [
    "#PASO 6: ENRIQUECIMIENTO DE DATOS\n",
    "\n",
    "#Mapa de precios conocidos\n",
    "price_map = {\n",
    "    \"Coffee\": 2.0, \"Tea\": 1.5, \"Sandwich\": 4.0, \"Salad\": 5.0,\n",
    "    \"Cake\": 3.0, \"Cookie\": 1.0, \"Smoothie\": 4.0, \"Juice\": 3.0\n",
    "}\n",
    "\n",
    "#Asignar precio esperado\n",
    "df[\"Expected Price\"] = df[\"Item\"].map(price_map)\n",
    "\n",
    "#Calcular desviación de precio\n",
    "df[\"Price Deviation\"] = df[\"Price Per Unit\"] - df[\"Expected Price\"]\n",
    "\n",
    "#Crear categoría de producto (bebida / comida)\n",
    "df[\"Category\"] = df[\"Item\"].apply(lambda x: \"Beverage\" if pd.notna(x) and x in [\"Coffee\", \"Tea\", \"Smoothie\", \"Juice\"] else \"Food\")\n",
    "\n",
    "#Mostrar ejemplo\n",
    "print(df[[\"Item\", \"Expected Price\", \"Price Per Unit\", \"Price Deviation\", \"Category\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2796184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo limpio exportado correctamente como 'clean_cafe_sales.csv'\n"
     ]
    }
   ],
   "source": [
    "#PASO 7: EXPORTAR DATASET LIMPIO\n",
    "\n",
    "#Eliminamos columna auxiliar\n",
    "df = df.drop(columns=[\"Calculated Total\"])\n",
    "\n",
    "#Guardar dataset limpio\n",
    "df.to_csv(\"clean_cafe_sales.csv\", index=False)\n",
    "\n",
    "print(\"Archivo limpio exportado correctamente como 'clean_cafe_sales.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06202c85",
   "metadata": {},
   "source": [
    "FASE 3: CARGA A MONGODB ATLAS\n",
    "\n",
    "Cargar los datos transformados desde el dataFrame a MongoDB Atlas, en una colección llamada cafe_sales_clean, dentro de una base de datos llamada etl_cafe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documentos insertados correctamente\n",
      "Total documentos en colección: 10000\n",
      "\\n Documento de ejemplo:\n",
      "{'_id': ObjectId('68f403225068f2321f61d093'), 'Transaction ID': 'TXN_1961373', 'Item': 'Coffee', 'Quantity': 2.0, 'Price Per Unit': 2.0, 'Total Spent': 4.0, 'Payment Method': 'Credit Card', 'Location': 'Takeaway', 'Transaction Date': '2023-09-08', 'Transaction Month': '2023-09', 'Day of Week': 'Friday', 'Expected Price': 2.0, 'Price Deviation': 0.0, 'Category': 'Beverage'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#Conexión cluster Atlas\n",
    "uri = \"mongodb+srv:CENSURADO=ClusterDataSciencieLAB\"\n",
    "\n",
    "#Crear cliente y base de datos\n",
    "client = MongoClient(uri)\n",
    "db = client[\"etl_cafe\"]\n",
    "collection = db[\"cafe_sales_clean\"]\n",
    "\n",
    "#Cargar el dataset limpio\n",
    "clean_df = pd.read_csv(\"clean_cafe_sales.csv\")\n",
    "\n",
    "#Convertir dataFrame a lista de diccionarios\n",
    "data_dict = clean_df.to_dict(\"records\")\n",
    "\n",
    "#Insertar datos en MongoDB\n",
    "if data_dict:\n",
    "    result = collection.insert_many(data_dict)\n",
    "    print(f\"{len(result.inserted_ids)} documentos insertados correctamente\")\n",
    "else:\n",
    "    print(\"No hay datos para insertar\")\n",
    "\n",
    "#Verificar la inserción\n",
    "print(f\"Total documentos en colección: {collection.count_documents({})}\")\n",
    "\n",
    "# Mostrar un documento de ejemplo\n",
    "print(\"Documento de ejemplo:\")\n",
    "print(collection.find_one())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
